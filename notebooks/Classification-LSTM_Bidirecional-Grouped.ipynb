{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6674ffe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "156/156 [==============================] - 127s 749ms/step - loss: 0.4507 - accuracy: 0.3451 - val_loss: 0.4376 - val_accuracy: 0.3478\n",
      "Epoch 2/10\n",
      "156/156 [==============================] - 94s 603ms/step - loss: 0.3761 - accuracy: 0.4263 - val_loss: 0.3823 - val_accuracy: 0.4547\n",
      "Epoch 3/10\n",
      "156/156 [==============================] - 86s 549ms/step - loss: 0.2811 - accuracy: 0.5673 - val_loss: 0.3949 - val_accuracy: 0.4674\n",
      "Epoch 4/10\n",
      "156/156 [==============================] - 87s 558ms/step - loss: 0.2068 - accuracy: 0.6475 - val_loss: 0.4537 - val_accuracy: 0.4583\n",
      "Epoch 5/10\n",
      "156/156 [==============================] - 93s 599ms/step - loss: 0.1560 - accuracy: 0.6839 - val_loss: 0.4912 - val_accuracy: 0.4330\n",
      "Epoch 6/10\n",
      "156/156 [==============================] - 100s 642ms/step - loss: 0.1225 - accuracy: 0.6956 - val_loss: 0.5672 - val_accuracy: 0.4257\n",
      "Epoch 7/10\n",
      "156/156 [==============================] - 92s 592ms/step - loss: 0.0968 - accuracy: 0.7107 - val_loss: 0.6179 - val_accuracy: 0.4438\n",
      "Epoch 8/10\n",
      "156/156 [==============================] - 90s 575ms/step - loss: 0.0770 - accuracy: 0.7137 - val_loss: 0.6750 - val_accuracy: 0.4366\n",
      "Epoch 9/10\n",
      "156/156 [==============================] - 96s 617ms/step - loss: 0.0607 - accuracy: 0.7101 - val_loss: 0.6974 - val_accuracy: 0.4257\n",
      "Epoch 10/10\n",
      "156/156 [==============================] - 95s 610ms/step - loss: 0.0513 - accuracy: 0.7095 - val_loss: 0.7307 - val_accuracy: 0.4239\n",
      "79/79 [==============================] - 16s 197ms/step - loss: 0.7955 - accuracy: 0.3912\n",
      "Loss: 0.7954996824264526\n",
      "Accuracy: 0.39121755957603455\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 1. Pré-processar os dados\n",
    "# Carregue seus datasets balanceados\n",
    "train_data = pd.read_csv('../data/preprocessed/movies_genres_grouped_train_preprocessed.csv')\n",
    "test_data = pd.read_csv('../data/preprocessed/movies_genres_grouped_test_preprocessed.csv')\n",
    "\n",
    "category_columns = train_data.columns.drop(['Name', 'Description', 'Combined'])\n",
    "\n",
    "# Defina os parâmetros de pré-processamento\n",
    "max_features = 10000  # Número máximo de palavras a serem usadas (palavras mais frequentes)\n",
    "maxlen = 200  # Número máximo de palavras no texto\n",
    "\n",
    "# Tokenize os textos\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_data['Combined'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_data['Combined'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['Combined'])\n",
    "\n",
    "# Padronize os textos\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# Obtenha os labels one-hot encoded\n",
    "y_train = train_data[category_columns].values\n",
    "y_test = test_data[category_columns].values\n",
    "\n",
    "# 2. Construir o modelo\n",
    "embedding_dim = 128  # Dimensão do vetor de embedding\n",
    "lstm_units = 64  # Unidades LSTM\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(max_features, embedding_dim, input_length=maxlen),\n",
    "    Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "    Bidirectional(LSTM(lstm_units)),\n",
    "    Dense(len(category_columns), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 3. Treinar o modelo\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# 4. Avaliar o modelo\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affa0801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 16s 182ms/step\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                 Comedy       0.57      0.59      0.58       862\n",
      "                  Crime       0.62      0.40      0.49       452\n",
      "                  Drama       0.72      0.60      0.65      1416\n",
      "                Romance       0.42      0.30      0.35       453\n",
      "   Action and Adventure       0.65      0.52      0.58       833\n",
      "Documentary and History       0.44      0.31      0.36       267\n",
      "   Family and Animation       0.52      0.39      0.45       271\n",
      "     Fantasy and Sci-Fi       0.51      0.34      0.41       320\n",
      "    Horror and Thriller       0.58      0.45      0.51       577\n",
      "\n",
      "              micro avg       0.61      0.49      0.54      5451\n",
      "              macro avg       0.56      0.43      0.49      5451\n",
      "           weighted avg       0.60      0.49      0.54      5451\n",
      "            samples avg       0.60      0.51      0.52      5451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prever e converter as previsões para o formato binarizado\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binarized = np.round(y_pred)\n",
    "\n",
    "# Calcular e exibir métricas de avaliação\n",
    "report = classification_report(y_test, y_pred_binarized, target_names=category_columns, zero_division=0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1132640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 13s 159ms/step\n",
      "F1-score (micro): 0.54\n",
      "F1-score (macro): 0.49\n",
      "Precision (micro): 0.61\n",
      "Precision (macro): 0.56\n",
      "Recall (micro): 0.49\n",
      "Recall (macro): 0.43\n",
      "79/79 [==============================] - 13s 159ms/step\n",
      "Class: Comedy\n",
      "F1-score: 0.58\n",
      "AUC-PR: 0.61\n",
      "\n",
      "\n",
      "Class: Crime\n",
      "F1-score: 0.49\n",
      "AUC-PR: 0.52\n",
      "\n",
      "\n",
      "Class: Drama\n",
      "F1-score: 0.65\n",
      "AUC-PR: 0.74\n",
      "\n",
      "\n",
      "Class: Romance\n",
      "F1-score: 0.35\n",
      "AUC-PR: 0.35\n",
      "\n",
      "\n",
      "Class: Action and Adventure\n",
      "F1-score: 0.58\n",
      "AUC-PR: 0.66\n",
      "\n",
      "\n",
      "Class: Documentary and History\n",
      "F1-score: 0.36\n",
      "AUC-PR: 0.33\n",
      "\n",
      "\n",
      "Class: Family and Animation\n",
      "F1-score: 0.45\n",
      "AUC-PR: 0.46\n",
      "\n",
      "\n",
      "Class: Fantasy and Sci-Fi\n",
      "F1-score: 0.41\n",
      "AUC-PR: 0.45\n",
      "\n",
      "\n",
      "Class: Horror and Thriller\n",
      "F1-score: 0.51\n",
      "AUC-PR: 0.54\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, average_precision_score, precision_recall_curve, precision_score, recall_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "print(\"F1-score (micro): {:.2f}\".format(f1_micro))\n",
    "print(\"F1-score (macro): {:.2f}\".format(f1_macro))\n",
    "print(\"Precision (micro): {:.2f}\".format(precision_micro))\n",
    "print(\"Precision (macro): {:.2f}\".format(precision_macro))\n",
    "print(\"Recall (micro): {:.2f}\".format(recall_micro))\n",
    "print(\"Recall (macro): {:.2f}\".format(recall_macro))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "f1_scores_per_class = f1_score(y_test, np.round(y_pred), average=None)\n",
    "auc_pr_per_class = average_precision_score(y_test, y_pred, average=None)\n",
    "for i, category in enumerate(category_columns):\n",
    "    print(\"Class: {}\".format(category))\n",
    "    print(\"F1-score: {:.2f}\".format(f1_scores_per_class[i]))\n",
    "    print(\"AUC-PR: {:.2f}\".format(auc_pr_per_class[i]))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
