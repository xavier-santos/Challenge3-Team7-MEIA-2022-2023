{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d35a6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "160/160 - 62s - loss: 0.5586 - precision_14: 0.5192 - recall_14: 0.1329 - val_loss: 0.5247 - val_precision_14: 0.5777 - val_recall_14: 0.2193 - 62s/epoch - 388ms/step\n",
      "Epoch 2/15\n",
      "160/160 - 43s - loss: 0.4795 - precision_14: 0.6310 - recall_14: 0.3830 - val_loss: 0.4368 - val_precision_14: 0.6759 - val_recall_14: 0.4755 - 43s/epoch - 268ms/step\n",
      "Epoch 3/15\n",
      "160/160 - 35s - loss: 0.3923 - precision_14: 0.7014 - recall_14: 0.5792 - val_loss: 0.4004 - val_precision_14: 0.6940 - val_recall_14: 0.5906 - 35s/epoch - 221ms/step\n",
      "Epoch 4/15\n",
      "160/160 - 41s - loss: 0.3234 - precision_14: 0.7612 - recall_14: 0.6918 - val_loss: 0.3730 - val_precision_14: 0.7144 - val_recall_14: 0.6345 - 41s/epoch - 254ms/step\n",
      "Epoch 5/15\n",
      "160/160 - 50s - loss: 0.2681 - precision_14: 0.8077 - recall_14: 0.7632 - val_loss: 0.3562 - val_precision_14: 0.7402 - val_recall_14: 0.6743 - 50s/epoch - 311ms/step\n",
      "Epoch 6/15\n",
      "160/160 - 47s - loss: 0.2314 - precision_14: 0.8346 - recall_14: 0.8023 - val_loss: 0.3597 - val_precision_14: 0.7420 - val_recall_14: 0.6807 - 47s/epoch - 295ms/step\n",
      "Epoch 7/15\n",
      "160/160 - 62s - loss: 0.1963 - precision_14: 0.8649 - recall_14: 0.8385 - val_loss: 0.3632 - val_precision_14: 0.7548 - val_recall_14: 0.6880 - 62s/epoch - 390ms/step\n",
      "Epoch 8/15\n",
      "160/160 - 44s - loss: 0.1728 - precision_14: 0.8840 - recall_14: 0.8577 - val_loss: 0.4002 - val_precision_14: 0.7526 - val_recall_14: 0.6855 - 44s/epoch - 278ms/step\n",
      "Epoch 9/15\n",
      "160/160 - 45s - loss: 0.1584 - precision_14: 0.8970 - recall_14: 0.8744 - val_loss: 0.3882 - val_precision_14: 0.7656 - val_recall_14: 0.7038 - 45s/epoch - 284ms/step\n",
      "Epoch 10/15\n",
      "160/160 - 70s - loss: 0.1465 - precision_14: 0.9084 - recall_14: 0.8862 - val_loss: 0.3758 - val_precision_14: 0.7841 - val_recall_14: 0.6810 - 70s/epoch - 438ms/step\n",
      "Epoch 11/15\n",
      "160/160 - 48s - loss: 0.1360 - precision_14: 0.9143 - recall_14: 0.8957 - val_loss: 0.3970 - val_precision_14: 0.7814 - val_recall_14: 0.7082 - 48s/epoch - 302ms/step\n",
      "Epoch 12/15\n",
      "160/160 - 68s - loss: 0.1232 - precision_14: 0.9239 - recall_14: 0.9078 - val_loss: 0.4389 - val_precision_14: 0.7702 - val_recall_14: 0.6954 - 68s/epoch - 423ms/step\n",
      "Epoch 13/15\n",
      "160/160 - 55s - loss: 0.1139 - precision_14: 0.9300 - recall_14: 0.9117 - val_loss: 0.4298 - val_precision_14: 0.7635 - val_recall_14: 0.7140 - 55s/epoch - 341ms/step\n",
      "Epoch 14/15\n",
      "160/160 - 46s - loss: 0.1012 - precision_14: 0.9371 - recall_14: 0.9252 - val_loss: 0.4423 - val_precision_14: 0.7739 - val_recall_14: 0.7201 - 46s/epoch - 288ms/step\n",
      "Epoch 15/15\n",
      "160/160 - 60s - loss: 0.0973 - precision_14: 0.9411 - recall_14: 0.9298 - val_loss: 0.3971 - val_precision_14: 0.7854 - val_recall_14: 0.7054 - 60s/epoch - 373ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_data = pd.read_csv('../data/balanced_train.csv')\n",
    "test_data = pd.read_csv('../data/balanced_test.csv')\n",
    "\n",
    "genre_columns = train_data.columns.drop(['Name', 'Description Tokenized'])\n",
    "\n",
    "# Tokenizar os textos\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_data['Description Tokenized']) + list(test_data['Description Tokenized']))\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_data['Description Tokenized'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['Description Tokenized'])\n",
    "\n",
    "maxlen = max(max([len(sequence) for sequence in X_train]), max([len(sequence) for sequence in X_test]))\n",
    "\n",
    "# Padronizar os textos\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "#labels one-hot encoded\n",
    "y_train = train_data[genre_columns].values\n",
    "y_test = test_data[genre_columns].values\n",
    "# Crie o modelo RNN simples com uma camada GRU\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, input_length=maxlen))\n",
    "model.add(GRU(128, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(genre_columns), activation='sigmoid'))\n",
    "\n",
    "# Compile e treine o modelo\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[Precision(), Recall()])\n",
    "\n",
    "num_epochs = 15\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c803284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 8s 79ms/step\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                 Comedy       0.72      0.72      0.72       487\n",
      "                  Crime       0.80      0.79      0.79       271\n",
      "                  Drama       0.82      0.72      0.76       613\n",
      "                Romance       0.86      0.51      0.64       270\n",
      "   Action and Adventure       0.82      0.73      0.77       463\n",
      "Documentary and History       0.72      0.67      0.69       172\n",
      "   Family and Animation       0.77      0.74      0.76       275\n",
      "     Fantasy and Sci-Fi       0.79      0.76      0.77       259\n",
      "    Horror and Thriller       0.77      0.65      0.70       309\n",
      "\n",
      "              micro avg       0.79      0.71      0.74      3119\n",
      "              macro avg       0.79      0.70      0.74      3119\n",
      "           weighted avg       0.79      0.71      0.74      3119\n",
      "            samples avg       0.72      0.69      0.69      3119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binarized = np.round(y_pred)\n",
    "\n",
    "\n",
    "report = classification_report(y_test, y_pred_binarized, target_names=genre_columns, zero_division=0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e3000e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 3s 76ms/step\n",
      "                     genre    AUC-PR   AUC-ROC\n",
      "0                   Comedy  0.836712  0.880782\n",
      "1                    Crime  0.855340  0.940698\n",
      "2                    Drama  0.866818  0.883721\n",
      "3                  Romance  0.780106  0.905728\n",
      "4     Action and Adventure  0.860132  0.908932\n",
      "5  Documentary and History  0.786522  0.947318\n",
      "6     Family and Animation  0.847249  0.930048\n",
      "7       Fantasy and Sci-Fi  0.853719  0.929730\n",
      "8      Horror and Thriller  0.775028  0.889214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "pr_curves = {}\n",
    "roc_curves = {}\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_prob[:, i])\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, i], y_pred_prob[:, i])\n",
    "    pr_curves[genre] = (precision, recall)\n",
    "    roc_curves[genre] = (fpr, tpr)\n",
    "    \n",
    "auc_pr = []\n",
    "auc_roc = []\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    ap = auc(pr_curves[genre][1], pr_curves[genre][0])\n",
    "    ar = auc(roc_curves[genre][0], roc_curves[genre][1])\n",
    "    auc_pr.append(ap)\n",
    "    auc_roc.append(ar)\n",
    "\n",
    "results = pd.DataFrame({'genre': genre_columns, 'AUC-PR': auc_pr, 'AUC-ROC': auc_roc})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca1d6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 4s 86ms/step\n",
      "Confusion matrix for Comedy:\n",
      "[[657 136]\n",
      " [134 353]]\n",
      "------------------------\n",
      "Confusion matrix for Crime:\n",
      "[[954  55]\n",
      " [ 56 215]]\n",
      "------------------------\n",
      "Confusion matrix for Drama:\n",
      "[[568  99]\n",
      " [173 440]]\n",
      "------------------------\n",
      "Confusion matrix for Romance:\n",
      "[[987  23]\n",
      " [132 138]]\n",
      "------------------------\n",
      "Confusion matrix for Action and Adventure:\n",
      "[[745  72]\n",
      " [125 338]]\n",
      "------------------------\n",
      "Confusion matrix for Documentary and History:\n",
      "[[1064   44]\n",
      " [  57  115]]\n",
      "------------------------\n",
      "Confusion matrix for Family and Animation:\n",
      "[[945  60]\n",
      " [ 71 204]]\n",
      "------------------------\n",
      "Confusion matrix for Fantasy and Sci-Fi:\n",
      "[[970  51]\n",
      " [ 63 196]]\n",
      "------------------------\n",
      "Confusion matrix for Horror and Thriller:\n",
      "[[910  61]\n",
      " [108 201]]\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Obter as previsões do modelo\n",
    "y_pred_binarized = np.round(model.predict(X_test))\n",
    "\n",
    "# Calcular a matriz de confusão para cada classe\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    cm = confusion_matrix(y_test[:, i], y_pred_binarized[:, i])\n",
    "    print(f'Confusion matrix for {genre}:')\n",
    "    print(cm)\n",
    "    print('------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
