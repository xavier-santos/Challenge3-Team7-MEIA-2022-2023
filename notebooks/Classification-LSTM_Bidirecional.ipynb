{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6674ffe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "144/144 [==============================] - 87s 353ms/step - loss: 0.5407 - accuracy: 0.2893 - val_loss: 0.4574 - val_accuracy: 0.3828\n",
      "Epoch 2/15\n",
      "144/144 [==============================] - 37s 256ms/step - loss: 0.3997 - accuracy: 0.4185 - val_loss: 0.3958 - val_accuracy: 0.4336\n",
      "Epoch 3/15\n",
      "144/144 [==============================] - 41s 288ms/step - loss: 0.2739 - accuracy: 0.4791 - val_loss: 0.3944 - val_accuracy: 0.4258\n",
      "Epoch 4/15\n",
      "144/144 [==============================] - 42s 294ms/step - loss: 0.1809 - accuracy: 0.4960 - val_loss: 0.4270 - val_accuracy: 0.4102\n",
      "Epoch 5/15\n",
      "144/144 [==============================] - 40s 277ms/step - loss: 0.1212 - accuracy: 0.5077 - val_loss: 0.4756 - val_accuracy: 0.3867\n",
      "Epoch 6/15\n",
      "144/144 [==============================] - 52s 363ms/step - loss: 0.0845 - accuracy: 0.4838 - val_loss: 0.5104 - val_accuracy: 0.4082\n",
      "Epoch 7/15\n",
      "144/144 [==============================] - 41s 286ms/step - loss: 0.0621 - accuracy: 0.4884 - val_loss: 0.5806 - val_accuracy: 0.4160\n",
      "Epoch 8/15\n",
      "144/144 [==============================] - 56s 389ms/step - loss: 0.0482 - accuracy: 0.4908 - val_loss: 0.6071 - val_accuracy: 0.3848\n",
      "Epoch 9/15\n",
      "144/144 [==============================] - 49s 342ms/step - loss: 0.0347 - accuracy: 0.4977 - val_loss: 0.6338 - val_accuracy: 0.4180\n",
      "Epoch 10/15\n",
      "144/144 [==============================] - 53s 372ms/step - loss: 0.0245 - accuracy: 0.4801 - val_loss: 0.6759 - val_accuracy: 0.3887\n",
      "Epoch 11/15\n",
      "144/144 [==============================] - 81s 565ms/step - loss: 0.0207 - accuracy: 0.4877 - val_loss: 0.7140 - val_accuracy: 0.3770\n",
      "Epoch 12/15\n",
      "144/144 [==============================] - 61s 422ms/step - loss: 0.0199 - accuracy: 0.4845 - val_loss: 0.7226 - val_accuracy: 0.4004\n",
      "Epoch 13/15\n",
      "144/144 [==============================] - 54s 376ms/step - loss: 0.0192 - accuracy: 0.5140 - val_loss: 0.7426 - val_accuracy: 0.3848\n",
      "Epoch 14/15\n",
      "144/144 [==============================] - 77s 538ms/step - loss: 0.0207 - accuracy: 0.5251 - val_loss: 0.7417 - val_accuracy: 0.3496\n",
      "Epoch 15/15\n",
      "144/144 [==============================] - 69s 478ms/step - loss: 0.0180 - accuracy: 0.5090 - val_loss: 0.7727 - val_accuracy: 0.4141\n",
      "40/40 [==============================] - 5s 124ms/step - loss: 0.7415 - accuracy: 0.3898\n",
      "Loss: 0.7414653897285461\n",
      "Accuracy: 0.38984376192092896\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "train_data = pd.read_csv('../data/balanced_train.csv')\n",
    "test_data = pd.read_csv('../data/balanced_test.csv')\n",
    "\n",
    "genre_columns = train_data.columns.drop(['Name', 'Description Tokenized'])\n",
    "\n",
    "# Tokenizar os textos\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_data['Description Tokenized']) + list(test_data['Description Tokenized']))\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_data['Description Tokenized'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['Description Tokenized'])\n",
    "\n",
    "maxlen = max(max([len(sequence) for sequence in X_train]), max([len(sequence) for sequence in X_test]))\n",
    "\n",
    "# Padronizar os textos\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "#labels one-hot encoded\n",
    "y_train = train_data[genre_columns].values\n",
    "y_test = test_data[genre_columns].values\n",
    "\n",
    "\n",
    "# 2. Construir o modelo\n",
    "embedding_dim = 128  # Dimens√£o do vetor de embedding\n",
    "lstm_units = 128  # Unidades LSTM\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(num_words, embedding_dim, input_length=maxlen),\n",
    "    Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "    Dropout(0.25),\n",
    "    Bidirectional(LSTM(lstm_units)),\n",
    "    Dense(len(genre_columns), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 3. Treinar o modelo\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# 4. Avaliar o modelo\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1132640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 33s 466ms/step\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                 Comedy       0.68      0.68      0.68       487\n",
      "                  Crime       0.83      0.68      0.75       271\n",
      "                  Drama       0.70      0.82      0.75       613\n",
      "                Romance       0.65      0.59      0.62       270\n",
      "   Action and Adventure       0.78      0.72      0.75       463\n",
      "Documentary and History       0.59      0.68      0.63       172\n",
      "   Family and Animation       0.73      0.65      0.69       275\n",
      "     Fantasy and Sci-Fi       0.74      0.74      0.74       259\n",
      "    Horror and Thriller       0.75      0.61      0.67       309\n",
      "\n",
      "              micro avg       0.72      0.70      0.71      3119\n",
      "              macro avg       0.72      0.69      0.70      3119\n",
      "           weighted avg       0.72      0.70      0.71      3119\n",
      "            samples avg       0.69      0.68      0.67      3119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binarized = np.round(y_pred)\n",
    "\n",
    "\n",
    "report = classification_report(y_test, y_pred_binarized, target_names=genre_columns, zero_division=0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8967b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 53ms/step\n",
      "                     genre    AUC-PR   AUC-ROC\n",
      "0                   Comedy  0.756734  0.849761\n",
      "1                    Crime  0.804367  0.907296\n",
      "2                    Drama  0.772113  0.831585\n",
      "3                  Romance  0.709490  0.880279\n",
      "4     Action and Adventure  0.789462  0.870685\n",
      "5  Documentary and History  0.696966  0.922629\n",
      "6     Family and Animation  0.771627  0.875535\n",
      "7       Fantasy and Sci-Fi  0.725032  0.895560\n",
      "8      Horror and Thriller  0.686137  0.866841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "pr_curves = {}\n",
    "roc_curves = {}\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_prob[:, i])\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, i], y_pred_prob[:, i])\n",
    "    pr_curves[genre] = (precision, recall)\n",
    "    roc_curves[genre] = (fpr, tpr)\n",
    "    \n",
    "auc_pr = []\n",
    "auc_roc = []\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    ap = auc(pr_curves[genre][1], pr_curves[genre][0])\n",
    "    ar = auc(roc_curves[genre][0], roc_curves[genre][1])\n",
    "    auc_pr.append(ap)\n",
    "    auc_roc.append(ar)\n",
    "\n",
    "results = pd.DataFrame({'genre': genre_columns, 'AUC-PR': auc_pr, 'AUC-ROC': auc_roc})\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a19dea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 38ms/step\n",
      "Confusion matrix for Comedy:\n",
      "[[640 153]\n",
      " [115 372]]\n",
      "------------------------\n",
      "Confusion matrix for Crime:\n",
      "[[945  64]\n",
      " [ 57 214]]\n",
      "------------------------\n",
      "Confusion matrix for Drama:\n",
      "[[541 126]\n",
      " [154 459]]\n",
      "------------------------\n",
      "Confusion matrix for Romance:\n",
      "[[925  85]\n",
      " [ 80 190]]\n",
      "------------------------\n",
      "Confusion matrix for Action and Adventure:\n",
      "[[718  99]\n",
      " [109 354]]\n",
      "------------------------\n",
      "Confusion matrix for Documentary and History:\n",
      "[[1064   44]\n",
      " [  65  107]]\n",
      "------------------------\n",
      "Confusion matrix for Family and Animation:\n",
      "[[951  54]\n",
      " [ 91 184]]\n",
      "------------------------\n",
      "Confusion matrix for Fantasy and Sci-Fi:\n",
      "[[932  89]\n",
      " [ 66 193]]\n",
      "------------------------\n",
      "Confusion matrix for Horror and Thriller:\n",
      "[[901  70]\n",
      " [112 197]]\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Obter as previs√µes do modelo\n",
    "y_pred_binarized = np.round(model.predict(X_test))\n",
    "\n",
    "# Calcular a matriz de confus√£o para cada classe\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    cm = confusion_matrix(y_test[:, i], y_pred_binarized[:, i])\n",
    "    print(f'Confusion matrix for {genre}:')\n",
    "    print(cm)\n",
    "    print('------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
