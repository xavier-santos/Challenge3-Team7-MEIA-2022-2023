{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6674ffe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "144/144 [==============================] - 87s 353ms/step - loss: 0.5407 - accuracy: 0.2893 - val_loss: 0.4574 - val_accuracy: 0.3828\n",
      "Epoch 2/15\n",
      "144/144 [==============================] - 37s 256ms/step - loss: 0.3997 - accuracy: 0.4185 - val_loss: 0.3958 - val_accuracy: 0.4336\n",
      "Epoch 3/15\n",
      "144/144 [==============================] - 41s 288ms/step - loss: 0.2739 - accuracy: 0.4791 - val_loss: 0.3944 - val_accuracy: 0.4258\n",
      "Epoch 4/15\n",
      "144/144 [==============================] - 42s 294ms/step - loss: 0.1809 - accuracy: 0.4960 - val_loss: 0.4270 - val_accuracy: 0.4102\n",
      "Epoch 5/15\n",
      "144/144 [==============================] - 40s 277ms/step - loss: 0.1212 - accuracy: 0.5077 - val_loss: 0.4756 - val_accuracy: 0.3867\n",
      "Epoch 6/15\n",
      "144/144 [==============================] - 52s 363ms/step - loss: 0.0845 - accuracy: 0.4838 - val_loss: 0.5104 - val_accuracy: 0.4082\n",
      "Epoch 7/15\n",
      "144/144 [==============================] - 41s 286ms/step - loss: 0.0621 - accuracy: 0.4884 - val_loss: 0.5806 - val_accuracy: 0.4160\n",
      "Epoch 8/15\n",
      "144/144 [==============================] - 56s 389ms/step - loss: 0.0482 - accuracy: 0.4908 - val_loss: 0.6071 - val_accuracy: 0.3848\n",
      "Epoch 9/15\n",
      "144/144 [==============================] - 49s 342ms/step - loss: 0.0347 - accuracy: 0.4977 - val_loss: 0.6338 - val_accuracy: 0.4180\n",
      "Epoch 10/15\n",
      "144/144 [==============================] - 53s 372ms/step - loss: 0.0245 - accuracy: 0.4801 - val_loss: 0.6759 - val_accuracy: 0.3887\n",
      "Epoch 11/15\n",
      "144/144 [==============================] - 81s 565ms/step - loss: 0.0207 - accuracy: 0.4877 - val_loss: 0.7140 - val_accuracy: 0.3770\n",
      "Epoch 12/15\n",
      "144/144 [==============================] - 61s 422ms/step - loss: 0.0199 - accuracy: 0.4845 - val_loss: 0.7226 - val_accuracy: 0.4004\n",
      "Epoch 13/15\n",
      "144/144 [==============================] - 54s 376ms/step - loss: 0.0192 - accuracy: 0.5140 - val_loss: 0.7426 - val_accuracy: 0.3848\n",
      "Epoch 14/15\n",
      "144/144 [==============================] - 77s 538ms/step - loss: 0.0207 - accuracy: 0.5251 - val_loss: 0.7417 - val_accuracy: 0.3496\n",
      "Epoch 15/15\n",
      "144/144 [==============================] - 69s 478ms/step - loss: 0.0180 - accuracy: 0.5090 - val_loss: 0.7727 - val_accuracy: 0.4141\n",
      "40/40 [==============================] - 5s 124ms/step - loss: 0.7415 - accuracy: 0.3898\n",
      "Loss: 0.7414653897285461\n",
      "Accuracy: 0.38984376192092896\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "train_data = pd.read_csv('../data/balanced_train.csv')\n",
    "test_data = pd.read_csv('../data/balanced_test.csv')\n",
    "\n",
    "genre_columns = train_data.columns.drop(['Name', 'Description Tokenized'])\n",
    "\n",
    "# Tokenizar os textos\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(train_data['Description Tokenized']) + list(test_data['Description Tokenized']))\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_data['Description Tokenized'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['Description Tokenized'])\n",
    "\n",
    "maxlen = max(max([len(sequence) for sequence in X_train]), max([len(sequence) for sequence in X_test]))\n",
    "\n",
    "# Padronizar os textos\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "#labels one-hot encoded\n",
    "y_train = train_data[genre_columns].values\n",
    "y_test = test_data[genre_columns].values\n",
    "\n",
    "\n",
    "# 2. Construir o modelo\n",
    "embedding_dim = 128  # Dimens√£o do vetor de embedding\n",
    "lstm_units = 128  # Unidades LSTM\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(num_words, embedding_dim, input_length=maxlen),\n",
    "    Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "    Dropout(0.25),\n",
    "    Bidirectional(LSTM(lstm_units)),\n",
    "    Dense(len(genre_columns), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 3. Treinar o modelo\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# 4. Avaliar o modelo\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1132640e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 16s 114ms/step\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                 Comedy       0.69      0.80      0.74       487\n",
      "                  Crime       0.75      0.81      0.78       271\n",
      "                  Drama       0.73      0.80      0.77       613\n",
      "                Romance       0.75      0.67      0.71       270\n",
      "   Action and Adventure       0.78      0.76      0.77       463\n",
      "Documentary and History       0.81      0.55      0.65       172\n",
      "   Family and Animation       0.72      0.71      0.71       275\n",
      "     Fantasy and Sci-Fi       0.76      0.69      0.72       259\n",
      "    Horror and Thriller       0.72      0.67      0.69       309\n",
      "\n",
      "              micro avg       0.74      0.74      0.74      3119\n",
      "              macro avg       0.74      0.72      0.73      3119\n",
      "           weighted avg       0.74      0.74      0.74      3119\n",
      "            samples avg       0.72      0.72      0.71      3119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binarized = np.round(y_pred)\n",
    "\n",
    "\n",
    "report = classification_report(y_test, y_pred_binarized, target_names=genre_columns, zero_division=0)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb1612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 7s 166ms/step\n",
      "                     genre    AUC-PR   AUC-ROC\n",
      "0                   Comedy  0.699929  0.840499\n",
      "1                    Crime  0.823727  0.917444\n",
      "2                    Drama  0.781922  0.824256\n",
      "3                  Romance  0.725966  0.870682\n",
      "4     Action and Adventure  0.787232  0.876115\n",
      "5  Documentary and History  0.723734  0.915566\n",
      "6     Family and Animation  0.721813  0.880213\n",
      "7       Fantasy and Sci-Fi  0.735604  0.879817\n",
      "8      Horror and Thriller  0.680893  0.859988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "pr_curves = {}\n",
    "roc_curves = {}\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_prob[:, i])\n",
    "    fpr, tpr, _ = roc_curve(y_test[:, i], y_pred_prob[:, i])\n",
    "    pr_curves[genre] = (precision, recall)\n",
    "    roc_curves[genre] = (fpr, tpr)\n",
    "    \n",
    "auc_pr = []\n",
    "auc_roc = []\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    ap = auc(pr_curves[genre][1], pr_curves[genre][0])\n",
    "    ar = auc(roc_curves[genre][0], roc_curves[genre][1])\n",
    "    auc_pr.append(ap)\n",
    "    auc_roc.append(ar)\n",
    "\n",
    "results = pd.DataFrame({'genre': genre_columns, 'AUC-PR': auc_pr, 'AUC-ROC': auc_roc})\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794b7879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 7s 185ms/step\n",
      "Confusion matrix for Comedy:\n",
      "[[617 176]\n",
      " [ 99 388]]\n",
      "------------------------\n",
      "Confusion matrix for Crime:\n",
      "[[935  74]\n",
      " [ 51 220]]\n",
      "------------------------\n",
      "Confusion matrix for Drama:\n",
      "[[489 178]\n",
      " [120 493]]\n",
      "------------------------\n",
      "Confusion matrix for Romance:\n",
      "[[948  62]\n",
      " [ 88 182]]\n",
      "------------------------\n",
      "Confusion matrix for Action and Adventure:\n",
      "[[718  99]\n",
      " [113 350]]\n",
      "------------------------\n",
      "Confusion matrix for Documentary and History:\n",
      "[[1086   22]\n",
      " [  78   94]]\n",
      "------------------------\n",
      "Confusion matrix for Family and Animation:\n",
      "[[929  76]\n",
      " [ 80 195]]\n",
      "------------------------\n",
      "Confusion matrix for Fantasy and Sci-Fi:\n",
      "[[963  58]\n",
      " [ 79 180]]\n",
      "------------------------\n",
      "Confusion matrix for Horror and Thriller:\n",
      "[[889  82]\n",
      " [103 206]]\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Obter as previs√µes do modelo\n",
    "y_pred_binarized = np.round(model.predict(X_test))\n",
    "\n",
    "# Calcular a matriz de confus√£o para cada classe\n",
    "for i, genre in enumerate(genre_columns):\n",
    "    cm = confusion_matrix(y_test[:, i], y_pred_binarized[:, i])\n",
    "    print(f'Confusion matrix for {genre}:')\n",
    "    print(cm)\n",
    "    print('------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
